{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from utils.helping_functions import save_results\n",
    "from utils.solution_generation import  read_matrix_form_jssp, generate_multiple_solutions\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "dev_map = f\"cuda:0\"\n",
    "# dev_map = f\"auto\"\n",
    "\n",
    "checkpoint_path = 'microsoft/Phi-3.5-mini-instruct'\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map = dev_map\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 1000\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True,)# add_bos_token=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.unk_token\n",
    "\n",
    "new_adapter_path = \"./testing_final_code_peft-phi3-Gcode_Generation_30k/checkpoint-302\"\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(model,\n",
    "                                  new_adapter_path,\n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  is_trainable=False,\n",
    "                                  device_map = dev_map,\n",
    "                \n",
    "                  )\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'prompt', 'response'],\n",
      "        num_rows: 30000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "custom_dataset_name = './gcode_prompt_response_data.json'\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=custom_dataset_name)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model= finetuned_model\n",
    "\n",
    "def gen(model,p, maxlen=1000, sample=False):\n",
    "    \"\"\"\n",
    "    Generates text using the model based on the provided prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained language model.\n",
    "        p (str): The prompt text.\n",
    "        maxlen (int, optional): Maximum length of the generated text. Defaults to 1000.\n",
    "        sample (bool, optional): Whether to use sampling. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = my_model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Provide a Gcode for following problem below.\n",
      "A rectangular plastic block of length 20mm, width 30mm and a height of 11mm is fixtured in a milling machine,   where the stock has to be milled on all 4 sides by 0mm. The top left corner is placed at (50, 2) in reference to the home postion of the machine   The top left corner needs a Chamfer of 8mm. The top right corner needs a Chamfer of 8mm. The bottom left corner needs a Chamfer of 4mm. The bottom right corner needs a Chamfer of 2mm.\n",
      "  A mill tool of diameter 8mm   having a side tooth engagement of 20mm which is greater than the thickness of stock is used for milling.\n",
      "  The bottom tip of the tool will be positioned 2.75mm below the part surface. \n",
      "The cutter will be operated at a spindle speed of 8000 RPM   and at a feed rate of 3000 mm/min.\n",
      "The TM1 mill cutter is in the location T6   with tool length stored in H04, tool diameter stored in D01\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "SOLUTION FROM GCODE Dataset Generator:\n",
      "%\n",
      "O30400 (Gcode for milling rectangular stock with chamfering/rounding) ;\n",
      "N1 G90 G80 G40 G49 G54 (Safe startup) ;\n",
      "N2 T6 M06 (Select tool T6);\n",
      "N3 G43 H04 (Activate tool length offset H04);\n",
      "N4 G42 D01 (Set tool diameter to 5mm);\n",
      "N5 S8000 M03 (Rotate spindle 8000RPM Clockwise);\n",
      "N6 G00 G54 (Rapid to work coordinate);\n",
      "N7 G01 Z13.75 F3000. (Feed to cutting depth) ;\n",
      "N8 X50 Y2 (at top left initial position);\n",
      "N9 X50 Y-28 ,C4 (at bottom left position);\n",
      "N10 X70 Y-28 ,C2 (at bottom right position);\n",
      "N11 X70 Y2 ,C8 (at top right position);\n",
      "N12 X50 Y2 ,R4 (at initial top left position);\n",
      "N13 G00 Z26.1 (Rapid Retract of Tool);\n",
      "N14 M05 (spindle off);\n",
      "N15 M30 (End Program);\n",
      "%\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "%\n",
      "O30407 (Gcode for milling rectangular stock with chamfering/rounding) ;\n",
      "N1 G90 G80 G40 G49 G54 (Safe startup) ;\n",
      "N2 T6 M06 (Select tool T6);\n",
      "N3 G43 H04 (Activate tool length offset H04);\n",
      "N4 G42 D01 (Set tool diameter to 5mm);\n",
      "N5 S8000 M03 (Rotate spindle 8000RPM Clockwise);\n",
      "N6 G00 Z29.75 (Rapid to initial height);\n",
      "N7 G01 X50 Y2 ,C8 (Move to top left initial position);\n",
      "N8 X50 Y-28 ,C4 (Move to bottom left position);\n",
      "N9 X70 Y-28 ,C2 (Move to bottom right position);\n",
      "N10 X70 Y2 ,C8 (Move to top right position);\n",
      "N11 X50 Y2 ,C4 (Round bottom left corner);\n",
      "N12 X50 Y2 ,C2 (Round bottom right corner);\n",
      "N13 G00 Z41.1 (Rapid Retract of Tool);\n",
      "N14 M05 (spindle off);\n",
      "N15 M30 (End Program);\n",
      "% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "prompt = dataset['train'][index]['prompt']\n",
    "summary = dataset['train'][index]['response']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Provide a Gcode for following problem below.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(my_model,formatted_prompt,1000,)\n",
    "# print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'SOLUTION FROM GCODE Dataset Generator:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
